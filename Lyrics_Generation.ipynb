{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyrics Generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "q2_cfX5tcPyQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,CuDNNLSTM,Bidirectional\n",
        "from keras.layers import Activation,Dropout,Flatten\n",
        "import keras.optimizers as optimizer\n",
        "import keras\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y0hH0L7wUD8X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOnhbZsrcePl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "temp = open('Kanye.txt',encoding = 'UTF-8').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P3IILf4Gc0Bu",
        "colab_type": "code",
        "outputId": "4b8c5318-1f2c-4b3f-ca93-11b61a3d785c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "temp[0:250]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDrug dealin\\' just to get by\\nStack ya\\' money \\'til it get sky high\\nWe wasn\\'t s\\'posed to make it past 25\\nJoke\\'s on you, we still alive\\nThrow your hands up in the sky and say:\\n\"\"We don\\'t care what people say\"\"\\n\\n[ 1]\\nIf this is your first time hearin\\' th'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "dw-tJWaqdxqw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "temp = temp.lower()\n",
        "# $ =  Enter\n",
        "temp = temp.replace('\\n','$')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jQt55uJFfs1N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lyrics = []\n",
        "for i,x in enumerate(temp):\n",
        "  if x == '[':\n",
        "    while(True):\n",
        "      if temp[i].strip() != ']':\n",
        "        i = i+1\n",
        "      else:\n",
        "        break\n",
        "  elif x.isdigit() or (x not in['$','?','.',' ',' \\ '] and not x.isalpha()):\n",
        "    pass\n",
        "  else:\n",
        "    lyrics.append(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMJITYHokLpm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(lyrics)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1l94ONTRmK1-",
        "colab_type": "code",
        "outputId": "8ad3ed64-44ef-4a7e-c029-a898e6058915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "JIz3zelJozwh",
        "colab_type": "code",
        "outputId": "9b0a764f-0fd4-4af8-f5ea-59d611e4849b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "str(chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[' ', '$', '.', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "vU00sMW7mwaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "intToChars = dict((i, c) for i, c in enumerate(chars))\n",
        "charsToInt = dict((i, c) for c, i in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "38danpiXn4_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = 20\n",
        "vocab_size = len(chars)\n",
        "lyrics_size = len(lyrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mr9TjFGwoQf6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "for i in range(0,lyrics_size - batch - 1):\n",
        "  tempX = lyrics[i:(i+batch)]\n",
        "  tempY = lyrics[i+batch]\n",
        "  \n",
        "  X.append([charsToInt[ch] for ch in tempX])\n",
        "  y.append(charsToInt[tempY])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r42AU0JMrAFa",
        "colab_type": "code",
        "outputId": "1f59cb0d-4b12-4e13-e3ac-41a46cb99f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "304573"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "EJMu4z8MOp_f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.reshape(X,(len(y),batch,1))\n",
        "y = np.reshape(y,(len(y),1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZbpG4U9vkjn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = X/30.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TsQYhis9PABq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = keras.utils.to_categorical(y, num_classes=30, dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NeAsUp41WbTU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CQfaeP3rS-cP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
        "model = Sequential()\n",
        "\n",
        "# add LSTM layer\n",
        "model.add(Bidirectional(CuDNNLSTM(128, input_shape=(batch, 1))))\n",
        "\n",
        "# add Softmax layer to output one character \n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# compile the model and pick the loss and optimizer\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.000001)\n",
        "\n",
        "filepath=\"weights-improvement-{epoch:02d}-{acc:.2f}.h5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "callbacks_list = [reduce_lr,checkpoint]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer='RMSprop')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jcx5AJt-URI2",
        "colab_type": "code",
        "outputId": "92f3be2e-5268-4b7a-acd6-8d7b2ab8e695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3410
        }
      },
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                              patience=5, min_lr=0.000000001)\n",
        "\n",
        "filepath=\"weights-improvement-{epoch:02d}-{acc:.2f}.h5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "callbacks_list = [reduce_lr,checkpoint]\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=100,validation_data = (X_test,y_test),verbose = 1,callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 274115 samples, validate on 30458 samples\n",
            "Epoch 1/100\n",
            "274115/274115 [==============================] - 33s 119us/step - loss: 2.7381 - acc: 0.2265 - val_loss: 2.6710 - val_acc: 0.2426\n",
            "Epoch 2/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 2.6034 - acc: 0.2578 - val_loss: 2.5043 - val_acc: 0.2827\n",
            "Epoch 3/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 2.4141 - acc: 0.3003 - val_loss: 2.3120 - val_acc: 0.3255\n",
            "Epoch 4/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 2.2545 - acc: 0.3392 - val_loss: 2.1479 - val_acc: 0.3686\n",
            "Epoch 5/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 2.1403 - acc: 0.3687 - val_loss: 2.1470 - val_acc: 0.3650\n",
            "Epoch 6/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 2.0525 - acc: 0.3899 - val_loss: 2.1234 - val_acc: 0.3722\n",
            "Epoch 7/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.9833 - acc: 0.4090 - val_loss: 1.9521 - val_acc: 0.4179\n",
            "Epoch 8/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.9276 - acc: 0.4244 - val_loss: 1.9008 - val_acc: 0.4338\n",
            "Epoch 9/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.8811 - acc: 0.4374 - val_loss: 1.8749 - val_acc: 0.4383\n",
            "Epoch 10/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.8418 - acc: 0.4476 - val_loss: 1.8427 - val_acc: 0.4464\n",
            "Epoch 11/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.8061 - acc: 0.4576 - val_loss: 1.8776 - val_acc: 0.4379\n",
            "Epoch 12/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.7774 - acc: 0.4654 - val_loss: 1.7913 - val_acc: 0.4646\n",
            "Epoch 13/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.7493 - acc: 0.4742 - val_loss: 1.7538 - val_acc: 0.4762\n",
            "Epoch 14/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.7222 - acc: 0.4810 - val_loss: 1.9408 - val_acc: 0.4167\n",
            "Epoch 15/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.7005 - acc: 0.4879 - val_loss: 1.7809 - val_acc: 0.4708\n",
            "Epoch 16/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.6770 - acc: 0.4943 - val_loss: 1.7297 - val_acc: 0.4837\n",
            "Epoch 17/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.6573 - acc: 0.5005 - val_loss: 1.7350 - val_acc: 0.4791\n",
            "Epoch 18/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.6397 - acc: 0.5062 - val_loss: 1.7073 - val_acc: 0.4904\n",
            "Epoch 19/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.6207 - acc: 0.5112 - val_loss: 1.7447 - val_acc: 0.4829\n",
            "Epoch 20/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.6049 - acc: 0.5155 - val_loss: 1.6924 - val_acc: 0.4945\n",
            "Epoch 21/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.5897 - acc: 0.5199 - val_loss: 1.7032 - val_acc: 0.4906\n",
            "Epoch 22/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.5752 - acc: 0.5241 - val_loss: 1.6993 - val_acc: 0.4927\n",
            "Epoch 23/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.5610 - acc: 0.5291 - val_loss: 1.7238 - val_acc: 0.4906\n",
            "Epoch 24/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.5484 - acc: 0.5325 - val_loss: 1.7331 - val_acc: 0.4866\n",
            "Epoch 25/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.5348 - acc: 0.5367 - val_loss: 1.6636 - val_acc: 0.5061\n",
            "Epoch 26/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.5237 - acc: 0.5395 - val_loss: 1.6813 - val_acc: 0.5032\n",
            "Epoch 27/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.5131 - acc: 0.5432 - val_loss: 1.7403 - val_acc: 0.4871\n",
            "Epoch 28/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.5025 - acc: 0.5462 - val_loss: 1.6611 - val_acc: 0.5093\n",
            "Epoch 29/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.4927 - acc: 0.5486 - val_loss: 1.7028 - val_acc: 0.4968\n",
            "Epoch 30/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.4828 - acc: 0.5509 - val_loss: 1.6742 - val_acc: 0.5078\n",
            "Epoch 31/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.4736 - acc: 0.5546 - val_loss: 1.6474 - val_acc: 0.5131\n",
            "Epoch 32/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.4643 - acc: 0.5573 - val_loss: 1.6527 - val_acc: 0.5105\n",
            "Epoch 33/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.4554 - acc: 0.5598 - val_loss: 1.6808 - val_acc: 0.5063\n",
            "Epoch 34/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.4480 - acc: 0.5621 - val_loss: 1.6646 - val_acc: 0.5069\n",
            "Epoch 35/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.4402 - acc: 0.5645 - val_loss: 1.6654 - val_acc: 0.5089\n",
            "Epoch 36/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.4314 - acc: 0.5673 - val_loss: 1.6360 - val_acc: 0.5189\n",
            "Epoch 37/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.4253 - acc: 0.5680 - val_loss: 1.7485 - val_acc: 0.4927\n",
            "Epoch 38/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.4173 - acc: 0.5715 - val_loss: 1.6868 - val_acc: 0.5050\n",
            "Epoch 39/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.4099 - acc: 0.5733 - val_loss: 1.6581 - val_acc: 0.5137\n",
            "Epoch 40/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.4046 - acc: 0.5746 - val_loss: 1.6405 - val_acc: 0.5180\n",
            "Epoch 41/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3976 - acc: 0.5775 - val_loss: 1.6314 - val_acc: 0.5223\n",
            "Epoch 42/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3917 - acc: 0.5780 - val_loss: 1.6494 - val_acc: 0.5152\n",
            "Epoch 43/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3853 - acc: 0.5800 - val_loss: 1.6672 - val_acc: 0.5103\n",
            "Epoch 44/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.3800 - acc: 0.5817 - val_loss: 1.6423 - val_acc: 0.5198\n",
            "Epoch 45/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3752 - acc: 0.5830 - val_loss: 1.6459 - val_acc: 0.5192\n",
            "Epoch 46/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.3709 - acc: 0.5842 - val_loss: 1.6324 - val_acc: 0.5218\n",
            "Epoch 47/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.3643 - acc: 0.5865 - val_loss: 1.6277 - val_acc: 0.5263\n",
            "Epoch 48/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3597 - acc: 0.5874 - val_loss: 1.6719 - val_acc: 0.5108\n",
            "Epoch 49/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3544 - acc: 0.5888 - val_loss: 1.6330 - val_acc: 0.5219\n",
            "Epoch 50/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.3485 - acc: 0.5908 - val_loss: 1.6559 - val_acc: 0.5180\n",
            "Epoch 51/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3447 - acc: 0.5913 - val_loss: 1.7072 - val_acc: 0.5044\n",
            "Epoch 52/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.3400 - acc: 0.5933 - val_loss: 1.6264 - val_acc: 0.5257\n",
            "Epoch 53/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3354 - acc: 0.5949 - val_loss: 1.8494 - val_acc: 0.4698\n",
            "Epoch 54/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3307 - acc: 0.5963 - val_loss: 1.6536 - val_acc: 0.5181\n",
            "Epoch 55/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3295 - acc: 0.5963 - val_loss: 1.9921 - val_acc: 0.4471\n",
            "Epoch 56/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.3246 - acc: 0.5987 - val_loss: 1.6282 - val_acc: 0.5212\n",
            "Epoch 57/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3185 - acc: 0.5997 - val_loss: 1.6511 - val_acc: 0.5200\n",
            "Epoch 58/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3162 - acc: 0.6005 - val_loss: 1.6580 - val_acc: 0.5215\n",
            "Epoch 59/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.3112 - acc: 0.6010 - val_loss: 1.6177 - val_acc: 0.5331\n",
            "Epoch 60/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.3089 - acc: 0.6028 - val_loss: 1.6293 - val_acc: 0.5245\n",
            "Epoch 61/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.3046 - acc: 0.6039 - val_loss: 1.6365 - val_acc: 0.5223\n",
            "Epoch 62/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.3014 - acc: 0.6054 - val_loss: 1.7635 - val_acc: 0.4953\n",
            "Epoch 63/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2997 - acc: 0.6044 - val_loss: 1.6234 - val_acc: 0.5291\n",
            "Epoch 64/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2954 - acc: 0.6052 - val_loss: 1.6396 - val_acc: 0.5267\n",
            "Epoch 65/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2921 - acc: 0.6063 - val_loss: 1.6247 - val_acc: 0.5313\n",
            "Epoch 66/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2894 - acc: 0.6078 - val_loss: 1.6093 - val_acc: 0.5339\n",
            "Epoch 67/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2848 - acc: 0.6091 - val_loss: 1.6406 - val_acc: 0.5289\n",
            "Epoch 68/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2822 - acc: 0.6098 - val_loss: 1.6433 - val_acc: 0.5276\n",
            "Epoch 69/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2801 - acc: 0.6106 - val_loss: 1.6100 - val_acc: 0.5380\n",
            "Epoch 70/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2761 - acc: 0.6120 - val_loss: 1.6402 - val_acc: 0.5307\n",
            "Epoch 71/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2744 - acc: 0.6122 - val_loss: 1.6340 - val_acc: 0.5311\n",
            "Epoch 72/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2709 - acc: 0.6132 - val_loss: 1.6195 - val_acc: 0.5377\n",
            "Epoch 73/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2672 - acc: 0.6139 - val_loss: 1.6201 - val_acc: 0.5330\n",
            "Epoch 74/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2656 - acc: 0.6150 - val_loss: 1.6406 - val_acc: 0.5300\n",
            "Epoch 75/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2623 - acc: 0.6154 - val_loss: 1.6270 - val_acc: 0.5343\n",
            "Epoch 76/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2594 - acc: 0.6163 - val_loss: 1.6700 - val_acc: 0.5199\n",
            "Epoch 77/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2578 - acc: 0.6175 - val_loss: 1.6490 - val_acc: 0.5267\n",
            "Epoch 78/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2548 - acc: 0.6171 - val_loss: 1.6966 - val_acc: 0.5189\n",
            "Epoch 79/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2520 - acc: 0.6181 - val_loss: 1.6236 - val_acc: 0.5356\n",
            "Epoch 80/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2496 - acc: 0.6188 - val_loss: 1.6400 - val_acc: 0.5321\n",
            "Epoch 81/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2464 - acc: 0.6197 - val_loss: 1.6476 - val_acc: 0.5311\n",
            "Epoch 82/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2440 - acc: 0.6217 - val_loss: 1.6244 - val_acc: 0.5375\n",
            "Epoch 83/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2419 - acc: 0.6214 - val_loss: 1.6552 - val_acc: 0.5278\n",
            "Epoch 84/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2382 - acc: 0.6229 - val_loss: 1.6249 - val_acc: 0.5343\n",
            "Epoch 85/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2393 - acc: 0.6224 - val_loss: 1.6256 - val_acc: 0.5357\n",
            "Epoch 86/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2362 - acc: 0.6236 - val_loss: 1.6756 - val_acc: 0.5241\n",
            "Epoch 87/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2348 - acc: 0.6241 - val_loss: 1.6180 - val_acc: 0.5430\n",
            "Epoch 88/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2318 - acc: 0.6244 - val_loss: 1.6285 - val_acc: 0.5403\n",
            "Epoch 89/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2292 - acc: 0.6248 - val_loss: 1.6828 - val_acc: 0.5271\n",
            "Epoch 90/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2268 - acc: 0.6257 - val_loss: 1.6343 - val_acc: 0.5362\n",
            "Epoch 91/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2254 - acc: 0.6266 - val_loss: 1.6712 - val_acc: 0.5252\n",
            "Epoch 92/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2231 - acc: 0.6265 - val_loss: 1.6305 - val_acc: 0.5377\n",
            "Epoch 93/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2208 - acc: 0.6264 - val_loss: 1.6367 - val_acc: 0.5364\n",
            "Epoch 94/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2202 - acc: 0.6278 - val_loss: 1.6360 - val_acc: 0.5364\n",
            "Epoch 95/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2170 - acc: 0.6277 - val_loss: 1.6460 - val_acc: 0.5352\n",
            "Epoch 96/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2153 - acc: 0.6287 - val_loss: 1.6295 - val_acc: 0.5379\n",
            "Epoch 97/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2139 - acc: 0.6293 - val_loss: 1.6628 - val_acc: 0.5355\n",
            "Epoch 98/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2118 - acc: 0.6303 - val_loss: 1.6435 - val_acc: 0.5362\n",
            "Epoch 99/100\n",
            "274115/274115 [==============================] - 32s 116us/step - loss: 1.2085 - acc: 0.6303 - val_loss: 1.6411 - val_acc: 0.5355\n",
            "Epoch 100/100\n",
            "274115/274115 [==============================] - 32s 117us/step - loss: 1.2073 - acc: 0.6312 - val_loss: 1.6430 - val_acc: 0.5364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe7f1be75c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "q1rRkUP_m9Y3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights('my_model_weights.h5')\n",
        "model.save('my_model.h5')\n",
        "\n",
        "files.download('my_model_weights.h5')\n",
        "files.download('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z73iDN4dmCAn",
        "colab_type": "code",
        "outputId": "ad0d0348-8271-4fa2-e88f-797b9b110a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "input_data = 'oh my god this fame '\n",
        "#input_data = 'waiting patiently i '\n",
        "\n",
        "print(input_data)\n",
        "for i in range(400):  \n",
        "  temp = [charsToInt[ch] for ch in input_data]\n",
        "  temp = np.asarray(temp)\n",
        "  temp = temp.reshape((1,20,1))\n",
        "  temp = temp/30 \n",
        "  hum = model.predict(temp)\n",
        "  hum = np.argmax(hum)\n",
        "  ch = intToChars[hum]\n",
        "  if ch != '$':\n",
        "    sys.stdout.write(ch)\n",
        "  else:\n",
        "    print('')\n",
        "  input_data = input_data[1:]\n",
        "  input_data = input_data + ch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "oh my god this fame \n",
            "in the alright sweats of you gon stupod on mass cant let me grees what i told hard if i could go for the be they want to in the sky and say\n",
            "we dont care what ment be wears take it the dont spok hollow\n",
            "\n",
            "hook kanye west  jayz\n",
            "this is in not just down on my nama than shit cont trying to cant set mothing good to my fault in a joes you will a pown\n",
            "and remember the charme the soul\n",
            "and they say and they "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}